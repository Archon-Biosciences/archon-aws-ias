{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc5-mbsX9PZC"
   },
   "source": [
    "# Using the AWS Batch Architecture for AlphaFold\n",
    "\n",
    "This notebook allows you to predict protein structures using AlphaFold on AWS Batch. \n",
    "\n",
    "**Differences to AlphaFold Notebooks**\n",
    "\n",
    "In comparison to AlphaFold v2.1.0, this notebook uses AWS Batch to submit protein analysis jobs to a scalable compute cluster. The accuracy should be the same as if you ran it locally. However, by using HPC services like AWS Batch and Amazon FSx for Lustre, we can support parallel job execution and optimize the resources for each run.\n",
    "\n",
    "**Citing this work**\n",
    "\n",
    "Any publication that discloses findings arising from using this notebook should [cite](https://github.com/deepmind/alphafold/#citing-this-work) the [AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2).\n",
    "\n",
    "**Licenses**\n",
    "\n",
    "Please refer to the `LICENSE` and `THIRD-PARTY-NOTICES` file for more information about third-party software/licensing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "0. [Install Dependencies](#0.-Install-Dependencies)\n",
    "1. [Run a monomer analysis job](#1.-Run-a-monomer-analysis-job)\n",
    "2. [Run a multimer analysis job](#2.-Run-a-multimer-analysis-job) \n",
    "3. [Analyze multiple proteins](#3.-Analyze-multiple-proteins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages\n",
    "# from Bio import SeqIO\n",
    "# from Bio.SeqRecord import SeqRecord\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "# from IPython import display\n",
    "# from nbhelpers import nbhelpers\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from time import sleep\n",
    "\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " S3 bucket name is sagemaker-us-east-2-032243382548\n"
     ]
    }
   ],
   "source": [
    "# Get client informatiion\n",
    "boto_session = boto3.session.Session(profile_name=\"bloyal+proteinfolding-Admin\")\n",
    "sm_session = sagemaker.session.Session(boto_session)\n",
    "region = boto_session.region_name\n",
    "s3_client = boto_session.client(\"s3\", region_name=region)\n",
    "batch_client = boto_session.client(\"batch\")\n",
    "\n",
    "S3_BUCKET = sm_session.default_bucket()\n",
    "print(f\" S3 bucket name is {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4JpOs6oA-QS"
   },
   "source": [
    "## 1. Run a monomer analysis job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sequences for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "rowN0bVYLe9n"
   },
   "outputs": [],
   "source": [
    "from batchfold.batchfold_target import BatchFoldTarget\n",
    "\n",
    "target = BatchFoldTarget(\n",
    "    target_id=\"T1084\", \n",
    "    s3_bucket=S3_BUCKET,\n",
    "    s3_base_prefix = \"T1084\",\n",
    "    boto_session=boto_session\n",
    "    ).add_sequence(\n",
    "        seq=\"MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\",\n",
    "        description=\"Meio, Meiothermus silvanus, 73 residues|\",\n",
    "    ).upload_fasta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchfold.batchfold_environment import BatchFoldEnvironment\n",
    "from batchfold.jackhmmer_job import JackhmmerJob\n",
    "from batchfold.openfold_job import OpenFoldJob\n",
    "\n",
    "batch_environment = BatchFoldEnvironment(boto_session = boto_session)\n",
    "\n",
    "jackhmmer_job = JackhmmerJob(\n",
    "    job_name = target.target_id + \"_JackhmmerJob_\" + datetime.now().strftime(\"%Y%m%d%s\"),\n",
    "    target_id = target.target_id,\n",
    "    fasta_s3_uri = target.get_fasta_s3_uri(),\n",
    "    output_s3_uri = target.get_msas_s3_uri(),\n",
    "    use_small_bfd = True,\n",
    "    boto_session = boto_session,\n",
    "    cpu = 4,\n",
    "    memory = 24\n",
    ")\n",
    "\n",
    "openfold_job = OpenFoldJob(\n",
    "    job_name = target.target_id + \"_OpenFoldJob_\" + datetime.now().strftime(\"%Y%m%d%s\"),\n",
    "    target_id = target.target_id,\n",
    "    fasta_s3_uri = target.get_fasta_s3_uri(),\n",
    "    msa_s3_uri = target.get_msas_s3_uri(),\n",
    "    output_s3_uri = target.get_predictions_s3_uri(),\n",
    "    use_precomputed_msas = True,\n",
    "    config_preset = \"finetuning_ptm\",\n",
    "    openfold_checkpoint_path = \"openfold_params/finetuning_ptm_1.pt\",\n",
    "    save_outputs = True,\n",
    "    boto_session = boto_session,\n",
    "    cpu = 4,\n",
    "    memory = 8,\n",
    "    gpu = 1\n",
    ")\n",
    "\n",
    "output = batch_environment \\\n",
    "    .submit_job(jackhmmer_job, job_queue_name=\"GravitonSpotJobQueue\") \\\n",
    "    .submit_job(openfold_job, job_queue_name=\"G4dnJobQueue\", dependent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'jobArn': 'arn:aws:batch:us-east-2:032243382548:job/2393fc52-a28d-4691-8334-5d627cd151a0',\n",
       "  'jobName': 'T1084_JackhmmerJob_202207141657836781',\n",
       "  'jobId': '2393fc52-a28d-4691-8334-5d627cd151a0',\n",
       "  'jobQueue': 'arn:aws:batch:us-east-2:032243382548:job-queue/GravitonSpotJobQueue-Ek1mq0xWLn72wHtg',\n",
       "  'status': 'SUBMITTED',\n",
       "  'attempts': [],\n",
       "  'createdAt': 1657836781378,\n",
       "  'retryStrategy': {'attempts': 3, 'evaluateOnExit': []},\n",
       "  'dependsOn': [],\n",
       "  'jobDefinition': 'arn:aws:batch:us-east-2:032243382548:job-definition/MSAJobDefinition-a3676b5d16ced49:1',\n",
       "  'parameters': {},\n",
       "  'container': {'image': '032243382548.dkr.ecr.us-east-2.amazonaws.com/batch-protein-folding-2206024-container-1vgix207fj09z-msacontainerregistry-ijkx3xxwmgxd:latest',\n",
       "   'command': ['aws s3 cp s3://sagemaker-us-east-2-032243382548/T1084/fastas/T1084.fasta /tmp/msa/fasta/ && python3 /opt/msa/create_alignments.py /tmp/msa/fasta --output_dir /tmp/msa/output --cpu 4 --mgnify_database_path /database/mgnify/mgy_clusters_2018_12.fa --pdb70_database_path /database/pdb70/pdb70 --uniclust30_database_path /database/uniclust30/uniclust30_2018_08/uniclust30_2018_08 --uniref90_database_path /database/uniref90/uniref90.fasta --bfd_database_path /database/small_bfd/bfd-first_non_consensus_sequences.fasta --use_small_bfd True && aws s3 cp --recursive /tmp/msa s3://sagemaker-us-east-2-032243382548/T1084/msas'],\n",
       "   'volumes': [{'host': {'sourcePath': '/fsx'}, 'name': 'database'}],\n",
       "   'environment': [],\n",
       "   'mountPoints': [{'containerPath': '/database',\n",
       "     'readOnly': True,\n",
       "     'sourceVolume': 'database'}],\n",
       "   'ulimits': [],\n",
       "   'networkInterfaces': [],\n",
       "   'resourceRequirements': [{'value': '4', 'type': 'VCPU'},\n",
       "    {'value': '24000', 'type': 'MEMORY'}],\n",
       "   'logConfiguration': {'logDriver': 'awslogs',\n",
       "    'options': {},\n",
       "    'secretOptions': []},\n",
       "   'secrets': []},\n",
       "  'tags': {},\n",
       "  'propagateTags': True,\n",
       "  'platformCapabilities': ['EC2']}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackhmmer_job.describe_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit two Batch jobs, the first one for the data prep and the second one (dependent on the first) for the structure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = status_1[\"jobName\"]\n",
    "# job_name = \"1234567890\"  # You can also provide the name of a previous job here\n",
    "nbhelpers.download_results(bucket=S3_BUCKET, job_name=job_name, local=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View MSA information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbhelpers.plot_msa_output_folder(\n",
    "    path=f\"data/{job_name}/{job_name}/msas\", id=input_ids[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View predicted structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_path = os.path.join(f\"data/{job_name}/{job_name}/ranked_0.pdb\")\n",
    "print(\"Default coloring is by plDDT\")\n",
    "nbhelpers.display_structure(pdb_path)\n",
    "\n",
    "print(\"Can also use rainbow coloring\")\n",
    "nbhelpers.display_structure(pdb_path, color=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4JpOs6oA-QS"
   },
   "source": [
    "## 2. Run a multimer analysis job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sequences for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rowN0bVYLe9n"
   },
   "outputs": [],
   "source": [
    "## Enter the amino acid sequence to fold\n",
    "\n",
    "id_1 = \"5ZNG_1\"\n",
    "sequence_1 = \"MDLSNMESVVESALTGQRTKIVVKVHMPCGKSRAKAMALAASVNGVDSVEITGEDKDRLVVVGRGIDPVRLVALLREKCGLAELLMVELVEKEKTQLAGGKKGAYKKHPTYNLSPFDYVEYPPSAPIMQDINPCSTM\"\n",
    "\n",
    "id_2 = \"5ZNG_2\"\n",
    "sequence_2 = (\n",
    "    \"MAWKDCIIQRYKDGDVNNIYTANRNEEITIEEYKVFVNEACHPYPVILPDRSVLSGDFTSAYADDDESCYRHHHHHH\"\n",
    ")\n",
    "\n",
    "# Add additional sequences, if necessary\n",
    "\n",
    "input_ids = (\n",
    "    id_1,\n",
    "    id_2,\n",
    ")\n",
    "input_sequences = (\n",
    "    sequence_1,\n",
    "    sequence_2,\n",
    ")\n",
    "\n",
    "DB_PRESET = \"reduced_dbs\"\n",
    "\n",
    "input_sequences, model_preset = nbhelpers.validate_input(input_sequences)\n",
    "sequence_length = len(max(input_sequences))\n",
    "\n",
    "# Upload input file to S3\n",
    "job_name = nbhelpers.create_job_name()\n",
    "object_key = nbhelpers.upload_fasta_to_s3(\n",
    "    input_sequences, input_ids, S3_BUCKET, job_name, region=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit batch jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resources for data prep and prediction steps\n",
    "if DB_PRESET == \"reduced_dbs\":\n",
    "    prep_cpu = 4\n",
    "    prep_mem = 16\n",
    "    prep_gpu = 0\n",
    "\n",
    "else:\n",
    "    prep_cpu = 16\n",
    "    prep_mem = 32\n",
    "    prep_gpu = 0\n",
    "\n",
    "if sequence_length < 700:\n",
    "    predict_cpu = 4\n",
    "    predict_mem = 16\n",
    "    predict_gpu = 1\n",
    "else:\n",
    "    predict_cpu = 16\n",
    "    predict_mem = 64\n",
    "    predict_gpu = 1\n",
    "\n",
    "step_1_response = nbhelpers.submit_batch_alphafold_job(\n",
    "    job_name=str(job_name),\n",
    "    fasta_paths=object_key,\n",
    "    output_dir=job_name,\n",
    "    db_preset=DB_PRESET,\n",
    "    model_preset=model_preset,\n",
    "    s3_bucket=S3_BUCKET,\n",
    "    cpu=prep_cpu,\n",
    "    memory=prep_mem,\n",
    "    gpu=prep_gpu,\n",
    "    use_spot_instances=True,\n",
    "    run_features_only=True,\n",
    ")\n",
    "\n",
    "print(f\"Job ID {step_1_response['jobId']} submitted\")\n",
    "\n",
    "step_2_response = nbhelpers.submit_batch_alphafold_job(\n",
    "    job_name=str(job_name),\n",
    "    fasta_paths=object_key,\n",
    "    output_dir=job_name,\n",
    "    db_preset=DB_PRESET,\n",
    "    model_preset=model_preset,\n",
    "    s3_bucket=S3_BUCKET,\n",
    "    cpu=predict_cpu,\n",
    "    memory=predict_mem,\n",
    "    gpu=predict_gpu,\n",
    "    features_paths=os.path.join(job_name, job_name, \"features.pkl\"),\n",
    "    depends_on=step_1_response[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(f\"Job ID {step_2_response['jobId']} submitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check status of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_1 = nbhelpers.get_batch_job_info(step_1_response[\"jobId\"])\n",
    "status_2 = nbhelpers.get_batch_job_info(step_2_response[\"jobId\"])\n",
    "\n",
    "print(f\"Data prep job {status_1['jobName']} is in {status_1['status']} status\")\n",
    "print(f\"Predict job {status_2['jobName']} is in {status_2['status']} status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download results from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = status_1[\"jobName\"]\n",
    "# job_name = \"1234567890\"  # You can also provide the name of a previous job here\n",
    "nbhelpers.download_results(bucket=S3_BUCKET, job_name=job_name, local=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View MSA information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbhelpers.plot_msa_output_folder(\n",
    "    path=f\"data/{job_name}/{job_name}/msas\", id=input_ids[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View predicted structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_path = os.path.join(f\"data/{job_name}/{job_name}/ranked_0.pdb\")\n",
    "print(\"Can also color by chain\")\n",
    "nbhelpers.display_structure(pdb_path, chains=2, color=\"chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze multiple proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and process CASP14 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://predictioncenter.org/download_area/CASP14/sequences/casp14.seq.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed '137,138d' \"casp14.seq.txt\" > \"casp14_dedup.fa\" # Remove duplicate entry for T1085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casp14_iterator = SeqIO.parse(\"casp14_dedup.fa\", \"fasta\")\n",
    "casp14_df = pd.DataFrame(\n",
    "    (\n",
    "        (record.id, record.description, len(record), record.seq)\n",
    "        for record in casp14_iterator\n",
    "    ),\n",
    "    columns=[\"id\", \"description\", \"length\", \"seq\"],\n",
    ").sort_values(by=\"length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display information about CASP14 proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display.display(casp14_df.loc[:, (\"id\", \"description\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution of the protein lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(casp14_df.length, bins=50)\n",
    "plt.ylabel(\"Sample count\")\n",
    "plt.xlabel(\"Residue count\")\n",
    "plt.title(\"CASP-14 Protein Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit analysis jobs for a subset of CASP14 proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_count = (\n",
    "    5  # Change this to analyze a larger number of CASP14 targets, smallest to largest\n",
    ")\n",
    "job_name_list = []\n",
    "\n",
    "for row in casp14_df[:protein_count].itertuples(index=False):\n",
    "    record = SeqRecord(row.seq, id=row.id, description=row.description)\n",
    "    print(f\"Protein sequence for analysis is \\n{record.description}\")\n",
    "    sequence_length = len(record.seq)\n",
    "    print(f\"Sequence length is {sequence_length}\")\n",
    "    print(record.seq)\n",
    "\n",
    "    input_ids = (record.id,)\n",
    "    input_sequences = (str(record.seq),)\n",
    "    DB_PRESET = \"reduced_dbs\"\n",
    "\n",
    "    input_sequences, model_preset = nbhelpers.validate_input(input_sequences)\n",
    "\n",
    "    # Upload input file to S3\n",
    "    job_name = nbhelpers.create_job_name()\n",
    "    object_key = nbhelpers.upload_fasta_to_s3(\n",
    "        input_sequences, input_ids, S3_BUCKET, job_name, region=region\n",
    "    )\n",
    "\n",
    "    # Define resources for data prep and prediction steps\n",
    "    if DB_PRESET == \"reduced_dbs\":\n",
    "        prep_cpu = 4\n",
    "        prep_mem = 16\n",
    "        prep_gpu = 0\n",
    "\n",
    "    else:\n",
    "        prep_cpu = 16\n",
    "        prep_mem = 32\n",
    "        prep_gpu = 0\n",
    "\n",
    "    if sequence_length < 700:\n",
    "        predict_cpu = 4\n",
    "        predict_mem = 16\n",
    "        predict_gpu = 1\n",
    "    else:\n",
    "        predict_cpu = 16\n",
    "        predict_mem = 64\n",
    "        predict_gpu = 1\n",
    "\n",
    "    step_1_response = nbhelpers.submit_batch_alphafold_job(\n",
    "        job_name=str(job_name),\n",
    "        fasta_paths=object_key,\n",
    "        output_dir=job_name,\n",
    "        db_preset=DB_PRESET,\n",
    "        model_preset=model_preset,\n",
    "        s3_bucket=S3_BUCKET,\n",
    "        cpu=prep_cpu,\n",
    "        memory=prep_mem,\n",
    "        gpu=prep_gpu,\n",
    "        run_features_only=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Job ID {step_1_response['jobId']} submitted\")\n",
    "\n",
    "    step_2_response = nbhelpers.submit_batch_alphafold_job(\n",
    "        job_name=str(job_name),\n",
    "        fasta_paths=object_key,\n",
    "        output_dir=job_name,\n",
    "        db_preset=DB_PRESET,\n",
    "        model_preset=model_preset,\n",
    "        s3_bucket=S3_BUCKET,\n",
    "        cpu=predict_cpu,\n",
    "        memory=predict_mem,\n",
    "        gpu=predict_gpu,\n",
    "        features_paths=os.path.join(job_name, job_name, \"features.pkl\"),\n",
    "        depends_on=step_1_response[\"jobId\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Job ID {step_2_response['jobId']} submitted\")\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AlphaFold.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1e60809b7f0a82b562ad10373da343921f7f222b56c39f24f5ac5a3d051206d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
